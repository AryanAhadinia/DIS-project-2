{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70aed8c4",
   "metadata": {
    "papermill": {
     "duration": 0.006231,
     "end_time": "2024-11-08T21:01:57.229600",
     "exception": false,
     "start_time": "2024-11-08T21:01:57.223369",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TEAM MAM Inference Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e44a10",
   "metadata": {
    "papermill": {
     "duration": 0.00545,
     "end_time": "2024-11-08T21:01:57.241016",
     "exception": false,
     "start_time": "2024-11-08T21:01:57.235566",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Depends\n",
    "\n",
    "1. On dis1-preprocess kaggle input which contains our prefitted models.\n",
    "2. On our GitHub reposity which contains the source code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50899ece",
   "metadata": {
    "papermill": {
     "duration": 0.005594,
     "end_time": "2024-11-08T21:01:57.252373",
     "exception": false,
     "start_time": "2024-11-08T21:01:57.246779",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Loading Source\n",
    "\n",
    "We first load our source code from our GitHub repository and then we install it as a library.\n",
    "\n",
    "Note: The token is used for the time that our repository was private. It can be ignored now since the repo is public."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f7391c5",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-08T21:01:57.266068Z",
     "iopub.status.busy": "2024-11-08T21:01:57.265607Z",
     "iopub.status.idle": "2024-11-08T21:01:59.609545Z",
     "shell.execute_reply": "2024-11-08T21:01:59.608066Z"
    },
    "papermill": {
     "duration": 2.354549,
     "end_time": "2024-11-08T21:01:59.612660",
     "exception": false,
     "start_time": "2024-11-08T21:01:57.258111",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'DIS_project1'...\r\n",
      "remote: Enumerating objects: 1411, done.\u001B[K\r\n",
      "remote: Counting objects: 100% (96/96), done.\u001B[K\r\n",
      "remote: Compressing objects: 100% (83/83), done.\u001B[K\r\n",
      "remote: Total 1411 (delta 38), reused 40 (delta 13), pack-reused 1315 (from 1)\u001B[K\r\n",
      "Receiving objects: 100% (1411/1411), 271.61 KiB | 7.99 MiB/s, done.\r\n",
      "Resolving deltas: 100% (900/900), done.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://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@github.com/madhueb/DIS_project1.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2884cea1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T21:01:59.629386Z",
     "iopub.status.busy": "2024-11-08T21:01:59.628952Z",
     "iopub.status.idle": "2024-11-08T21:04:13.169301Z",
     "shell.execute_reply": "2024-11-08T21:04:13.167725Z"
    },
    "papermill": {
     "duration": 133.552716,
     "end_time": "2024-11-08T21:04:13.172744",
     "exception": false,
     "start_time": "2024-11-08T21:01:59.620028",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting camel-tools\r\n",
      "  Downloading camel_tools-1.5.5-py3-none-any.whl.metadata (10 kB)\r\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.10/site-packages (from camel-tools) (1.0.0)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from camel-tools) (1.16.0)\r\n",
      "Requirement already satisfied: docopt in /opt/conda/lib/python3.10/site-packages (from camel-tools) (0.6.2)\r\n",
      "Requirement already satisfied: cachetools in /opt/conda/lib/python3.10/site-packages (from camel-tools) (4.2.4)\r\n",
      "Requirement already satisfied: numpy<2 in /opt/conda/lib/python3.10/site-packages (from camel-tools) (1.26.4)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from camel-tools) (1.14.1)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from camel-tools) (2.2.3)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from camel-tools) (1.2.2)\r\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from camel-tools) (0.3.8)\r\n",
      "Requirement already satisfied: torch>=2.0 in /opt/conda/lib/python3.10/site-packages (from camel-tools) (2.4.0+cpu)\r\n",
      "Collecting transformers<4.44.0,>=4.0 (from camel-tools)\r\n",
      "  Downloading transformers-4.43.4-py3-none-any.whl.metadata (43 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m43.7/43.7 kB\u001B[0m \u001B[31m1.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting editdistance (from camel-tools)\r\n",
      "  Downloading editdistance-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from camel-tools) (2.32.3)\r\n",
      "Requirement already satisfied: emoji in /opt/conda/lib/python3.10/site-packages (from camel-tools) (2.13.2)\r\n",
      "Collecting pyrsistent (from camel-tools)\r\n",
      "  Downloading pyrsistent-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\r\n",
      "Requirement already satisfied: tabulate in /opt/conda/lib/python3.10/site-packages (from camel-tools) (0.9.0)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from camel-tools) (4.66.4)\r\n",
      "Collecting muddler (from camel-tools)\r\n",
      "  Downloading muddler-0.1.3-py3-none-any.whl.metadata (7.5 kB)\r\n",
      "Collecting camel-kenlm>=2024.5.6 (from camel-tools)\r\n",
      "  Downloading camel-kenlm-2024.5.6.zip (556 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m556.4/556.4 kB\u001B[0m \u001B[31m14.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25h  Installing build dependencies ... \u001B[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \bdone\r\n",
      "\u001B[?25h  Getting requirements to build wheel ... \u001B[?25l-\b \bdone\r\n",
      "\u001B[?25h  Preparing metadata (pyproject.toml) ... \u001B[?25l-\b \bdone\r\n",
      "\u001B[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=2.0->camel-tools) (3.15.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0->camel-tools) (4.12.2)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=2.0->camel-tools) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=2.0->camel-tools) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0->camel-tools) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=2.0->camel-tools) (2024.6.1)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers<4.44.0,>=4.0->camel-tools) (0.25.1)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers<4.44.0,>=4.0->camel-tools) (21.3)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers<4.44.0,>=4.0->camel-tools) (6.0.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<4.44.0,>=4.0->camel-tools) (2024.5.15)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<4.44.0,>=4.0->camel-tools) (0.4.5)\r\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers<4.44.0,>=4.0->camel-tools)\r\n",
      "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->camel-tools) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->camel-tools) (2024.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->camel-tools) (2024.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->camel-tools) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->camel-tools) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->camel-tools) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->camel-tools) (2024.8.30)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->camel-tools) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->camel-tools) (3.5.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers<4.44.0,>=4.0->camel-tools) (3.1.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=2.0->camel-tools) (2.1.5)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=2.0->camel-tools) (1.3.0)\r\n",
      "Downloading camel_tools-1.5.5-py3-none-any.whl (124 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m124.5/124.5 kB\u001B[0m \u001B[31m5.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading transformers-4.43.4-py3-none-any.whl (9.4 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m9.4/9.4 MB\u001B[0m \u001B[31m79.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading editdistance-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (401 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m401.8/401.8 kB\u001B[0m \u001B[31m16.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading muddler-0.1.3-py3-none-any.whl (16 kB)\r\n",
      "Downloading pyrsistent-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (117 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m117.7/117.7 kB\u001B[0m \u001B[31m5.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.6/3.6 MB\u001B[0m \u001B[31m59.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hBuilding wheels for collected packages: camel-kenlm\r\n",
      "  Building wheel for camel-kenlm (pyproject.toml) ... \u001B[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \bdone\r\n",
      "\u001B[?25h  Created wheel for camel-kenlm: filename=camel_kenlm-2024.5.6-cp310-cp310-linux_x86_64.whl size=592943 sha256=47085777aad3907302a26306a1080189c7a2ce4788076040f4aadeba2961c563\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/2b/93/ff/ac84dae74c91ffe3e1c344a71f991946eacc79eada61cb703f\r\n",
      "Successfully built camel-kenlm\r\n",
      "Installing collected packages: camel-kenlm, pyrsistent, muddler, editdistance, tokenizers, transformers, camel-tools\r\n",
      "  Attempting uninstall: tokenizers\r\n",
      "    Found existing installation: tokenizers 0.20.0\r\n",
      "    Uninstalling tokenizers-0.20.0:\r\n",
      "      Successfully uninstalled tokenizers-0.20.0\r\n",
      "  Attempting uninstall: transformers\r\n",
      "    Found existing installation: transformers 4.45.1\r\n",
      "    Uninstalling transformers-4.45.1:\r\n",
      "      Successfully uninstalled transformers-4.45.1\r\n",
      "Successfully installed camel-kenlm-2024.5.6 camel-tools-1.5.5 editdistance-0.8.1 muddler-0.1.3 pyrsistent-0.20.0 tokenizers-0.19.1 transformers-4.43.4\r\n",
      "The following packages will be installed: 'morphology-db-msa-r13', 'disambig-mle-calima-msa-r13'\r\n",
      "Downloading package 'morphology-db-msa-r13': 100%|\u001B[32m█\u001B[0m| 40.5M/40.5M [00:00<00:00, 1\u001B[0m\r\n",
      "Extracting package 'morphology-db-msa-r13': 100%|\u001B[32m█\u001B[0m| 40.5M/40.5M [00:00<00:00, 47\u001B[0m\r\n",
      "Downloading package 'disambig-mle-calima-msa-r13': 100%|\u001B[32m█\u001B[0m| 88.7M/88.7M [00:00<00\u001B[0m\r\n",
      "Extracting package 'disambig-mle-calima-msa-r13': 100%|\u001B[32m█\u001B[0m| 88.7M/88.7M [00:00<00:\u001B[0m\r\n",
      "Obtaining file:///kaggle/working/DIS_project1\r\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hInstalling collected packages: src\r\n",
      "  Running setup.py develop for src\r\n",
      "Successfully installed src-0.0.0\r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('./DIS_project1')\n",
    "!pip install camel-tools\n",
    "!camel_data -i disambig-mle-calima-msa-r13\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d40f241f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T21:04:13.217443Z",
     "iopub.status.busy": "2024-11-08T21:04:13.216943Z",
     "iopub.status.idle": "2024-11-08T21:04:14.407235Z",
     "shell.execute_reply": "2024-11-08T21:04:14.405597Z"
    },
    "papermill": {
     "duration": 1.215995,
     "end_time": "2024-11-08T21:04:14.410366",
     "exception": false,
     "start_time": "2024-11-08T21:04:13.194371",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import gc\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c2b522",
   "metadata": {
    "papermill": {
     "duration": 0.01998,
     "end_time": "2024-11-08T21:04:14.451023",
     "exception": false,
     "start_time": "2024-11-08T21:04:14.431043",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Loading the Data and Prefitted Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8986b889",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T21:04:14.492832Z",
     "iopub.status.busy": "2024-11-08T21:04:14.492250Z",
     "iopub.status.idle": "2024-11-08T21:06:15.320036Z",
     "shell.execute_reply": "2024-11-08T21:06:15.318639Z"
    },
    "papermill": {
     "duration": 120.852048,
     "end_time": "2024-11-08T21:06:15.323089",
     "exception": false,
     "start_time": "2024-11-08T21:04:14.471041",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fr-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.8.0/fr_core_news_sm-3.8.0-py3-none-any.whl (16.3 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m16.3/16.3 MB\u001B[0m \u001B[31m16.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: fr-core-news-sm\n",
      "Successfully installed fr-core-news-sm-3.8.0\n",
      "\u001B[38;5;2m✔ Download and installation successful\u001B[0m\n",
      "You can now load the package via spacy.load('fr_core_news_sm')\n",
      "\u001B[38;5;3m⚠ Restart to reload dependencies\u001B[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n",
      "Collecting de-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.8.0/de_core_news_sm-3.8.0-py3-none-any.whl (14.6 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m14.6/14.6 MB\u001B[0m \u001B[31m43.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: de-core-news-sm\n",
      "Successfully installed de-core-news-sm-3.8.0\n",
      "\u001B[38;5;2m✔ Download and installation successful\u001B[0m\n",
      "You can now load the package via spacy.load('de_core_news_sm')\n",
      "\u001B[38;5;3m⚠ Restart to reload dependencies\u001B[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n",
      "Collecting it-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/it_core_news_sm-3.8.0/it_core_news_sm-3.8.0-py3-none-any.whl (13.0 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m13.0/13.0 MB\u001B[0m \u001B[31m30.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: it-core-news-sm\n",
      "Successfully installed it-core-news-sm-3.8.0\n",
      "\u001B[38;5;2m✔ Download and installation successful\u001B[0m\n",
      "You can now load the package via spacy.load('it_core_news_sm')\n",
      "\u001B[38;5;3m⚠ Restart to reload dependencies\u001B[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n",
      "Collecting es-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.8.0/es_core_news_sm-3.8.0-py3-none-any.whl (12.9 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m12.9/12.9 MB\u001B[0m \u001B[31m50.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: es-core-news-sm\n",
      "Successfully installed es-core-news-sm-3.8.0\n",
      "\u001B[38;5;2m✔ Download and installation successful\u001B[0m\n",
      "You can now load the package via spacy.load('es_core_news_sm')\n",
      "\u001B[38;5;3m⚠ Restart to reload dependencies\u001B[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n",
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Collecting ko-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/ko_core_news_sm-3.8.0/ko_core_news_sm-3.8.0-py3-none-any.whl (14.7 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m14.7/14.7 MB\u001B[0m \u001B[31m16.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: ko-core-news-sm\n",
      "Successfully installed ko-core-news-sm-3.8.0\n",
      "\u001B[38;5;2m✔ Download and installation successful\u001B[0m\n",
      "You can now load the package via spacy.load('ko_core_news_sm')\n",
      "\u001B[38;5;3m⚠ Restart to reload dependencies\u001B[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m12.8/12.8 MB\u001B[0m \u001B[31m61.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h\u001B[38;5;2m✔ Download and installation successful\u001B[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001B[38;5;3m⚠ Restart to reload dependencies\u001B[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "is_test = True\n",
    "query_path = '/kaggle/input/dis-project-1-document-retrieval'\n",
    "tokens_path = '/kaggle/input/dis1-preprocess/doc_tokens'\n",
    "models_path = '/kaggle/input/dis1-preprocess/models'\n",
    "doc_ids_path = '/kaggle/input/dis1-preprocess/ids_dict.json'\n",
    "out_path = '/kaggle/working/'\n",
    "LANGS = [\"fr\", \"de\", \"it\", \"es\", \"ar\", \"ko\", \"en\"]\n",
    "\n",
    "from src.bm25_tfidf.text_tokenizer import (\n",
    "    FrenchTokenizer,\n",
    "    EnglishTokenizer,\n",
    "    GermanTokenizer,\n",
    "    ItalianTokenizer,\n",
    "    SpanishTokenizer,\n",
    "    ArabicTokenizer,\n",
    "    KoreanTokenizer\n",
    ")\n",
    "\n",
    "tokenizers = {\"fr\": FrenchTokenizer(), \"de\": GermanTokenizer(), \"it\": ItalianTokenizer(), \"es\": SpanishTokenizer(),\n",
    "              \"ar\": ArabicTokenizer(), \"ko\": KoreanTokenizer(), \"en\": EnglishTokenizer()}\n",
    "\n",
    "\n",
    "# load doc ids dict with json\n",
    "with open(doc_ids_path, \"r\") as f:\n",
    "    ids_dict = json.load(f)\n",
    "\n",
    "for lang in LANGS:\n",
    "    ids_dict[lang] = np.array(ids_dict[lang])\n",
    "\n",
    "mode = 'bm25_tfidf'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c04d6b",
   "metadata": {
    "papermill": {
     "duration": 0.02344,
     "end_time": "2024-11-08T21:06:15.370264",
     "exception": false,
     "start_time": "2024-11-08T21:06:15.346824",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Depending on the running mode, we load dev or test data. The model will be evaluated if you decide the dev set to be loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c01633a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T21:06:15.422151Z",
     "iopub.status.busy": "2024-11-08T21:06:15.421348Z",
     "iopub.status.idle": "2024-11-08T21:06:15.455755Z",
     "shell.execute_reply": "2024-11-08T21:06:15.454514Z"
    },
    "papermill": {
     "duration": 0.064876,
     "end_time": "2024-11-08T21:06:15.458699",
     "exception": false,
     "start_time": "2024-11-08T21:06:15.393823",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if is_test:\n",
    "    queries = pd.read_csv(f'{query_path}/test.csv')\n",
    "else:\n",
    "    queries = pd.read_csv(f'{query_path}/dev.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3df633",
   "metadata": {
    "papermill": {
     "duration": 0.023365,
     "end_time": "2024-11-08T21:06:15.506040",
     "exception": false,
     "start_time": "2024-11-08T21:06:15.482675",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## BM25\n",
    "\n",
    "We predict the related document for each query for BM25 model here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0377ab8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T21:06:15.557051Z",
     "iopub.status.busy": "2024-11-08T21:06:15.556557Z",
     "iopub.status.idle": "2024-11-08T21:07:21.902301Z",
     "shell.execute_reply": "2024-11-08T21:07:21.900807Z"
    },
    "papermill": {
     "duration": 66.406451,
     "end_time": "2024-11-08T21:07:21.937448",
     "exception": false,
     "start_time": "2024-11-08T21:06:15.530997",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:01, 102.51it/s]\n",
      "200it [00:01, 164.39it/s]\n",
      "200it [00:01, 169.75it/s]\n",
      "200it [00:01, 149.32it/s]\n",
      "200it [00:01, 165.63it/s]\n",
      "800it [00:03, 228.00it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25s = {}\n",
    "\n",
    "for lang in LANGS:\n",
    "    with open(f\"{models_path}/bm25_{lang}.pkl\", \"rb\") as f:\n",
    "        bm25s[lang] = pickle.load(f)\n",
    "\n",
    "\n",
    "ls = [[] for _ in range(len(queries))]\n",
    "queries[\"bm25_docids\"] = ls\n",
    "for lang in LANGS:\n",
    "    if is_test:\n",
    "        queries_lang = queries[queries[\"lang\"] == lang][[\"query\"]].reset_index(drop=True)\n",
    "    else:\n",
    "        queries_lang = queries[queries[\"lang\"] == lang][[\"query\", \"positive_docs\"]].reset_index(drop=True)\n",
    "    tokens = tokenizers[lang].tokenize(queries_lang[\"query\"].tolist())\n",
    "    bm25_ind = bm25s[lang]\n",
    "    doc_ids = []\n",
    "    for tokenized_query in tokens:\n",
    "        indices, _ = bm25_ind.match(tokenized_query, k=10)\n",
    "        doc_ids.append(ids_dict[lang][indices].tolist())\n",
    "\n",
    "    queries.loc[queries[\"lang\"] == lang, \"bm25_docids\"] = pd.Series(doc_ids, index=queries.loc[queries[\"lang\"] == lang].index)\n",
    "    if not is_test:\n",
    "        acc = 0\n",
    "        for i, row in queries_lang.iterrows():\n",
    "            if row[\"positive_docs\"] in doc_ids[i]:\n",
    "                acc += 1\n",
    "        print(f\"Accuracy for {lang} : {acc / len(queries_lang)}\")\n",
    "    gc.collect()\n",
    "\n",
    "if not is_test:\n",
    "    acc = 0\n",
    "    for i, row in queries.iterrows():\n",
    "        if row[\"positive_docs\"] in row[\"bm25_docids\"]:\n",
    "            acc += 1\n",
    "    print(f\"Accuracy for all : {acc / len(queries)}\")\n",
    "\n",
    "if mode == 'bm25' and is_test:\n",
    "    queries.rename(columns={'bm25_docids': 'docids'}, inplace=True)\n",
    "    queries = queries[[\"id\", \"docids\"]]\n",
    "    queries.to_csv(f\"{out_path}/submission.csv\", index=False)\n",
    "    print('submission created')\n",
    "    exit(0)\n",
    "if mode == 'bm25':\n",
    "    exit(0)\n",
    "\n",
    "del bm25s\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05063df",
   "metadata": {
    "papermill": {
     "duration": 0.032829,
     "end_time": "2024-11-08T21:07:22.003359",
     "exception": false,
     "start_time": "2024-11-08T21:07:21.970530",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## TF-IDF\n",
    "\n",
    "We predict the related document for each query for TF-IDF model here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6d0a9b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T21:07:22.071525Z",
     "iopub.status.busy": "2024-11-08T21:07:22.071037Z",
     "iopub.status.idle": "2024-11-08T21:09:02.333670Z",
     "shell.execute_reply": "2024-11-08T21:09:02.332191Z"
    },
    "papermill": {
     "duration": 100.343713,
     "end_time": "2024-11-08T21:09:02.380275",
     "exception": false,
     "start_time": "2024-11-08T21:07:22.036562",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:02, 96.02it/s] \n",
      "200it [00:00, 6830.34it/s]\n",
      "200it [00:01, 153.12it/s]\n",
      "200it [00:00, 7325.08it/s]\n",
      "200it [00:01, 155.45it/s]\n",
      "200it [00:00, 12905.55it/s]\n",
      "200it [00:01, 143.14it/s]\n",
      "200it [00:00, 13488.89it/s]\n",
      "200it [00:00, 22880.93it/s]\n",
      "200it [00:01, 156.58it/s]\n",
      "200it [00:00, 17657.63it/s]\n",
      "800it [00:03, 228.50it/s]\n",
      "800it [00:00, 31501.78it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfs = {}\n",
    "for lang in LANGS:\n",
    "    with open(f\"{models_path}/tfidf_{lang}.pkl\", \"rb\") as f:\n",
    "        tfidfs[lang] = pickle.load(f)\n",
    "\n",
    "ls = [[] for _ in range(len(queries))]\n",
    "queries[\"tfidf_docids\"] = ls\n",
    "for lang in LANGS:\n",
    "    if is_test:\n",
    "        queries_lang = queries[queries[\"lang\"] == lang][[\"query\"]].reset_index(drop=True)\n",
    "    else:\n",
    "        queries_lang = queries[queries[\"lang\"] == lang][[\"query\", \"positive_docs\"]].reset_index(drop=True)\n",
    "    \n",
    "    tokens = tokenizers[lang].tokenize([query for query in queries_lang[\"query\"].tolist()])\n",
    "    ids_ = tfidfs[lang].retrieve_top_k(tokens, k=10)\n",
    "    doc_ids = [ids_dict[lang][doc_id].tolist() for doc_id in ids_]\n",
    "    queries.loc[queries[\"lang\"] == lang, \"tfidf_docids\"] = pd.Series(doc_ids,\n",
    "                                                               index=queries.loc[queries[\"lang\"] == lang].index)\n",
    "\n",
    "    if not is_test:\n",
    "        acc = 0\n",
    "        for i, row in queries_lang.iterrows():\n",
    "            if row[\"positive_docs\"] in doc_ids[i]:\n",
    "                acc += 1\n",
    "        print(f\"Accuracy for {lang} : {acc / len(queries_lang)}\")\n",
    "    gc.collect()\n",
    "if not is_test:\n",
    "    acc = 0\n",
    "    for i, row in queries.iterrows():\n",
    "        if row[\"positive_docs\"] in row[\"tfidf_docids\"]:\n",
    "            acc += 1\n",
    "    print(f\"Accuracy for all : {acc / len(queries)}\")\n",
    "\n",
    "if mode == 'tfidf' and is_test:\n",
    "    queries.rename(columns={'tfidf_docids': 'docids'}, inplace=True)\n",
    "    queries = queries[[\"id\", \"docids\"]]\n",
    "    queries.to_csv(f\"{out_path}/submission.csv\", index=False)\n",
    "    print('submission created')\n",
    "    exit(0)\n",
    "\n",
    "if mode == 'tfidf':\n",
    "    exit(0)\n",
    "\n",
    "del tfidfs\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865ce749",
   "metadata": {
    "papermill": {
     "duration": 0.056099,
     "end_time": "2024-11-08T21:09:02.483707",
     "exception": false,
     "start_time": "2024-11-08T21:09:02.427608",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Ensemble Model\n",
    "\n",
    "We predict the related document for each query for ensemble model here.\n",
    "\n",
    "We use pretuned portion of contribution for combining the models. It can be tuned using `is_tune` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bdf2562",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-08T21:09:02.568944Z",
     "iopub.status.busy": "2024-11-08T21:09:02.568445Z",
     "iopub.status.idle": "2024-11-08T21:09:02.783890Z",
     "shell.execute_reply": "2024-11-08T21:09:02.782470Z"
    },
    "papermill": {
     "duration": 0.262062,
     "end_time": "2024-11-08T21:09:02.786879",
     "exception": false,
     "start_time": "2024-11-08T21:09:02.524817",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission created\n"
     ]
    }
   ],
   "source": [
    "ls = [[] for _ in range(len(queries))]\n",
    "queries[\"docids\"] = ls\n",
    "\n",
    "k = 10\n",
    "\n",
    "is_tune = False\n",
    "\n",
    "bm25_ind_doc_ids = queries['bm25_docids'].tolist()\n",
    "\n",
    "tfidf_doc_ids = queries['tfidf_docids'].tolist()\n",
    "if is_tune:\n",
    "    langs_kb = {lang: [] for lang in LANGS}\n",
    "    for k_b in range(k+1):\n",
    "        print(f'dev on k_b {k_b}')\n",
    "        doc_ids = [] \n",
    "        for i in range(len(bm25_ind_doc_ids)):\n",
    "            docid = bm25_ind_doc_ids[i][:k_b]\n",
    "            for rec in tfidf_doc_ids[i]:\n",
    "                if len(docid) == k:\n",
    "                    break\n",
    "                if rec not in docid:\n",
    "                    docid.append(rec)\n",
    "            l = k_b\n",
    "            while len(docid) < k:\n",
    "                if bm25_ind_doc_ids[i][l] not in docid:\n",
    "                    docid.append(bm25_ind_doc_ids[i][l])\n",
    "                l += 1\n",
    "            doc_ids.append(docid)\n",
    "        \n",
    "        queries[\"docids\"] = pd.Series(doc_ids)\n",
    "        \n",
    "        for lang in LANGS:\n",
    "            queries_lang = queries[queries[\"lang\"] == lang][[\"query\", \"positive_docs\", \"docids\"]].reset_index(drop=True)\n",
    "            acc = 0\n",
    "            for i, row in queries_lang.iterrows():\n",
    "                if row[\"positive_docs\"] in row[\"docids\"]:\n",
    "                    acc += 1\n",
    "            print(f\"Accuracy for {lang} : {acc / len(queries_lang)}\")\n",
    "            langs_kb[lang].append(acc / len(queries_lang))\n",
    "        acc = 0\n",
    "        for i, row in queries.iterrows():\n",
    "            if row[\"positive_docs\"] in row[\"docids\"]:\n",
    "                acc += 1\n",
    "        print(f\"Accuracy for all : {acc / len(queries)}\")\n",
    "        gc.collect()\n",
    "    for lang in LANGS:\n",
    "        lang_arr = np.array(langs_kb[lang])\n",
    "        max_value = np.max(lang_arr)\n",
    "        ind_ = np.where(lang_arr == max_value)[0]\n",
    "        print(f'kb amx for {lang}: {ind_[-1]}')\n",
    "\n",
    "else:\n",
    "    \n",
    "    doc_ids = [] \n",
    "    kbs = {'fr': 9,\n",
    "            'de': 10,\n",
    "            'it': 10,\n",
    "            'es': 8,\n",
    "            'ar': 9,\n",
    "            'ko': 8,\n",
    "            'en': 8\n",
    "          }\n",
    "    for i in range(len(bm25_ind_doc_ids)):\n",
    "        k_b = kbs[queries.iloc[i]['lang']]\n",
    "        docid = bm25_ind_doc_ids[i][:k_b]\n",
    "        for rec in tfidf_doc_ids[i]:\n",
    "            if len(docid) == k:\n",
    "                break\n",
    "            if rec not in docid:\n",
    "                docid.append(rec)\n",
    "        l = k_b\n",
    "        while len(docid) < k:\n",
    "            if bm25_ind_doc_ids[i][l] not in docid:\n",
    "                docid.append(bm25_ind_doc_ids[i][l])\n",
    "            l += 1\n",
    "        doc_ids.append(docid)\n",
    "    queries[\"docids\"] = pd.Series(doc_ids)\n",
    "    if is_test:\n",
    "        queries = queries[[\"id\", \"docids\"]]\n",
    "        queries.to_csv(f\"{out_path}/submission.csv\", index=False)\n",
    "        print('submission created')\n",
    "    else:\n",
    "        for lang in LANGS:\n",
    "            queries_lang = queries[queries[\"lang\"] == lang][[\"query\", \"positive_docs\", \"docids\"]].reset_index(drop=True)\n",
    "            acc = 0\n",
    "            for i, row in queries_lang.iterrows():\n",
    "                if row[\"positive_docs\"] in row[\"docids\"]:\n",
    "                    acc += 1\n",
    "            print(f\"Accuracy for {lang} : {acc / len(queries_lang)}\")\n",
    "        acc = 0\n",
    "        for i, row in queries.iterrows():\n",
    "            if row[\"positive_docs\"] in row[\"docids\"]:\n",
    "                acc += 1\n",
    "        print(f\"Accuracy for all : {acc / len(queries)}\")\n",
    "        gc.collect()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 9635715,
     "sourceId": 85316,
     "sourceType": "competition"
    },
    {
     "datasetId": 6032096,
     "sourceId": 9834337,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 431.663371,
   "end_time": "2024-11-08T21:09:05.450534",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-08T21:01:53.787163",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
